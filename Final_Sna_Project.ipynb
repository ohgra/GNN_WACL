{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drPJwL6xaJBk",
        "outputId": "2efafd3b-1af0-4239-b2e9-e33d1a0331a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.datasets import Actor\n",
        "from torch_geometric.datasets import Amazon\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "class CustomGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(CustomGNNLayer, self).__init__(aggr='add')\n",
        "\n",
        "        self.Wd = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.Wp = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.Wk = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.Wg = nn.Parameter(torch.randn(3))\n",
        "        self.Wmlp = nn.Parameter(torch.randn(out_features, out_features))\n",
        "\n",
        "        nn.init.xavier_uniform_(self.Wd)\n",
        "        nn.init.xavier_uniform_(self.Wp)\n",
        "        nn.init.xavier_uniform_(self.Wk)\n",
        "        nn.init.xavier_uniform_(self.Wmlp)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X, A, sqrtD, sqrtP, sqrtK):\n",
        "        H1 = sqrtD @ A @ sqrtD @ X @ self.Wd  # n * f\n",
        "        H2 = sqrtP @ A @ sqrtP @ X @ self.Wp  # n * f\n",
        "        H3 = sqrtK @ A @ sqrtK @ X @ self.Wk  # n * f\n",
        "\n",
        "        H_combined = torch.stack([H1, H2, H3], dim=-1)  # n * f * 3\n",
        "\n",
        "        H_multiplied = H_combined * self.Wg  # n * f * 3\n",
        "\n",
        "        H_summed = torch.sum(H_multiplied, dim=-1)  # n * f\n",
        "\n",
        "        output = H_summed @ self.Wmlp  # n * f\n",
        "\n",
        "        return self.relu(output)\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.gnn_layer1 = CustomGNNLayer(in_features, hidden_features)\n",
        "        self.gnn_layer2 = CustomGNNLayer(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, X, A, sqrtD, sqrtP, sqrtK):\n",
        "        H = self.gnn_layer1(X, A, sqrtD, sqrtP, sqrtK)\n",
        "        output = self.gnn_layer2(H, A, sqrtD, sqrtP, sqrtK)\n",
        "        return F.log_softmax(output, dim=1)\n",
        "\n",
        "def compute_matrices(graph):\n",
        "    n = graph.number_of_nodes()\n",
        "\n",
        "    A = torch.tensor(nx.adjacency_matrix(graph).todense(), dtype=torch.float32)\n",
        "    A += torch.eye(n)\n",
        "\n",
        "    degrees = torch.tensor([graph.degree(i) for i in range(n)], dtype=torch.float32)\n",
        "    D = torch.diag(degrees)\n",
        "    sqrtD = torch.diag(torch.sqrt(degrees + 5e-6))\n",
        "\n",
        "    pagerank_scores = nx.pagerank(graph)\n",
        "    pagerank_vector = torch.tensor([pagerank_scores[i] for i in range(n)], dtype=torch.float32)\n",
        "    P = torch.diag(pagerank_vector)\n",
        "    sqrtP = torch.diag(torch.sqrt(pagerank_vector + 5e-6))\n",
        "\n",
        "    katz_scores = nx.katz_centrality(graph, alpha=0.01, max_iter=5000, tol=1e-6)\n",
        "    katz_vector = torch.tensor([katz_scores[i] for i in range(n)], dtype=torch.float32)\n",
        "    K = torch.diag(katz_vector)\n",
        "    sqrtK = torch.diag(torch.sqrt(katz_vector + 5e-6))\n",
        "\n",
        "    return A, sqrtD, sqrtP, sqrtK\n",
        "\n",
        "def load_cora_data():\n",
        "    #dataset = Actor(root='/tmp/Actor')\n",
        "    #dataset = Amazon(root='/tmp/Amazon-Computers', name='Computers')\n",
        "    dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "    data = dataset[0]\n",
        "\n",
        "    graph = to_networkx(data, to_undirected=True)\n",
        "\n",
        "    A, sqrtD, sqrtP, sqrtK = compute_matrices(graph)\n",
        "\n",
        "    return data.x, A, sqrtD, sqrtP, sqrtK, data.y, data.train_mask, data.val_mask, data.test_mask\n",
        "\n",
        "def accuracy(preds, labels):\n",
        "    correct = (preds == labels).sum().item()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def train(model, data, optimizer, criterion, epochs=100, device='cpu', model_save_path='best_model.pth'):\n",
        "    X, A, sqrtD, sqrtP, sqrtK, labels, train_mask, val_mask, test_mask = data\n",
        "    X, A, sqrtD, sqrtP, sqrtK, labels = X.to(device), A.to(device), sqrtD.to(device), sqrtP.to(device), sqrtK.to(device), labels.to(device)\n",
        "    train_mask, val_mask, test_mask = train_mask.to(device), val_mask.to(device), test_mask.to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(X, A, sqrtD, sqrtP, sqrtK)\n",
        "        predictions = output.argmax(dim=1)\n",
        "        train_loss = criterion(output[train_mask], labels[train_mask])\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_acc = accuracy(predictions[train_mask], labels[train_mask])\n",
        "        val_acc = accuracy(predictions[val_mask], labels[val_mask])\n",
        "        test_acc = accuracy(predictions[test_mask], labels[test_mask])\n",
        "\n",
        "\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss.item():.4f}, '\n",
        "                  f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f} '\n",
        "                  f'(Best Test Acc: {best_test_acc:.4f})')\n",
        "\n",
        "def evaluate_model_on_graph(graph, model_save_path, in_features, hidden_features, out_features, device='cpu'):\n",
        "    X, A, sqrtD, sqrtP, sqrtK = compute_matrices(graph)\n",
        "    X, A, sqrtD, sqrtP, sqrtK = X.to(device), A.to(device), sqrtD.to(device), sqrtP.to(device), sqrtK.to(device)\n",
        "\n",
        "\n",
        "    model = GNNModel(in_features=in_features, hidden_features=hidden_features, out_features=out_features)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(X, A, sqrtD, sqrtP, sqrtK)\n",
        "        predictions = output.argmax(dim=1)\n",
        "\n",
        "    return predictions.cpu().numpy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    set_seed(80)\n",
        "\n",
        "    epochs = 1000\n",
        "    learning_rate = 0.0005\n",
        "    weight_decay = 8e-3\n",
        "    hidden_features = 2048\n",
        "    model_save_path = 'best_gnn_model.pth'\n",
        "    X, A, sqrtD, sqrtP, sqrtK, labels, train_mask, val_mask, test_mask = load_cora_data()\n",
        "    in_features = X.shape[1]\n",
        "    out_features = labels.max().item() + 1\n",
        "    model = GNNModel(in_features=in_features, hidden_features=hidden_features, out_features=out_features)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train(model, (X, A, sqrtD, sqrtP, sqrtK, labels, train_mask, val_mask, test_mask), optimizer, criterion, epochs, device, model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrcD01KEaqzT",
        "outputId": "f92dde90-a9bb-4778-83bb-b4e7728a7649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 1751.5820, Train Acc: 0.5071, Val Acc: 0.3460, Test Acc: 0.3630 (Best Test Acc: 0.3940)\n",
            "Epoch [20/1000], Loss: 440.3495, Train Acc: 0.7571, Val Acc: 0.5740, Test Acc: 0.6080 (Best Test Acc: 0.6080)\n",
            "Epoch [30/1000], Loss: 1827.6302, Train Acc: 0.7643, Val Acc: 0.6080, Test Acc: 0.6100 (Best Test Acc: 0.6180)\n",
            "Epoch [40/1000], Loss: 1296.6155, Train Acc: 0.7214, Val Acc: 0.5740, Test Acc: 0.5960 (Best Test Acc: 0.6650)\n",
            "Epoch [50/1000], Loss: 825.4711, Train Acc: 0.8500, Val Acc: 0.6900, Test Acc: 0.6770 (Best Test Acc: 0.6780)\n",
            "Epoch [60/1000], Loss: 510.4550, Train Acc: 0.8429, Val Acc: 0.6720, Test Acc: 0.6770 (Best Test Acc: 0.7080)\n",
            "Epoch [70/1000], Loss: 166.2802, Train Acc: 0.9000, Val Acc: 0.6900, Test Acc: 0.6980 (Best Test Acc: 0.7080)\n",
            "Epoch [80/1000], Loss: 12.9639, Train Acc: 0.8929, Val Acc: 0.6680, Test Acc: 0.6790 (Best Test Acc: 0.7080)\n",
            "Epoch [90/1000], Loss: 2.3340, Train Acc: 0.8643, Val Acc: 0.6120, Test Acc: 0.6290 (Best Test Acc: 0.7080)\n",
            "Epoch [100/1000], Loss: 1.1866, Train Acc: 0.9143, Val Acc: 0.6380, Test Acc: 0.6730 (Best Test Acc: 0.7080)\n",
            "Epoch [110/1000], Loss: 0.3793, Train Acc: 0.9571, Val Acc: 0.6780, Test Acc: 0.7070 (Best Test Acc: 0.7120)\n",
            "Epoch [120/1000], Loss: 0.0745, Train Acc: 0.9714, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [130/1000], Loss: 0.0702, Train Acc: 0.9714, Val Acc: 0.6760, Test Acc: 0.6990 (Best Test Acc: 0.7120)\n",
            "Epoch [140/1000], Loss: 0.0682, Train Acc: 0.9714, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [150/1000], Loss: 0.4191, Train Acc: 0.9643, Val Acc: 0.6740, Test Acc: 0.6950 (Best Test Acc: 0.7120)\n",
            "Epoch [160/1000], Loss: 0.0646, Train Acc: 0.9714, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [170/1000], Loss: 0.0632, Train Acc: 0.9714, Val Acc: 0.6740, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [180/1000], Loss: 0.0619, Train Acc: 0.9714, Val Acc: 0.6720, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [190/1000], Loss: 0.0608, Train Acc: 0.9714, Val Acc: 0.6720, Test Acc: 0.6950 (Best Test Acc: 0.7120)\n",
            "Epoch [200/1000], Loss: 0.0599, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [210/1000], Loss: 0.0590, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [220/1000], Loss: 0.0582, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [230/1000], Loss: 0.0575, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [240/1000], Loss: 0.0569, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [250/1000], Loss: 0.0562, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [260/1000], Loss: 0.0556, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [270/1000], Loss: 0.0551, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [280/1000], Loss: 0.0546, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [290/1000], Loss: 0.0541, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [300/1000], Loss: 0.0536, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [310/1000], Loss: 0.0532, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [320/1000], Loss: 0.0528, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [330/1000], Loss: 0.0524, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [340/1000], Loss: 0.0520, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [350/1000], Loss: 0.0517, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [360/1000], Loss: 0.0513, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [370/1000], Loss: 0.0510, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [380/1000], Loss: 0.0507, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [390/1000], Loss: 0.0504, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [400/1000], Loss: 0.0501, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [410/1000], Loss: 0.0499, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [420/1000], Loss: 0.0496, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [430/1000], Loss: 0.0494, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [440/1000], Loss: 0.0492, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [450/1000], Loss: 0.0490, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [460/1000], Loss: 0.0488, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [470/1000], Loss: 0.0486, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [480/1000], Loss: 0.0484, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [490/1000], Loss: 0.0482, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [500/1000], Loss: 0.0480, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [510/1000], Loss: 0.0479, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [520/1000], Loss: 0.0477, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [530/1000], Loss: 0.0476, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [540/1000], Loss: 0.0474, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [550/1000], Loss: 0.0473, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [560/1000], Loss: 0.0472, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [570/1000], Loss: 0.0470, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [580/1000], Loss: 0.0469, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [590/1000], Loss: 0.0468, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [600/1000], Loss: 0.0467, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [610/1000], Loss: 0.0466, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [620/1000], Loss: 0.0465, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [630/1000], Loss: 0.0464, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [640/1000], Loss: 0.0463, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [650/1000], Loss: 0.0462, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [660/1000], Loss: 0.0461, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [670/1000], Loss: 0.0461, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [680/1000], Loss: 0.0460, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [690/1000], Loss: 0.0459, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [700/1000], Loss: 0.0458, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [710/1000], Loss: 0.0458, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [720/1000], Loss: 0.0457, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [730/1000], Loss: 0.0456, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [740/1000], Loss: 0.0456, Train Acc: 0.9786, Val Acc: 0.6720, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [750/1000], Loss: 0.0455, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [760/1000], Loss: 0.0455, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [770/1000], Loss: 0.0454, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [780/1000], Loss: 0.0453, Train Acc: 0.9786, Val Acc: 0.6740, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [790/1000], Loss: 0.0453, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [800/1000], Loss: 0.0452, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [810/1000], Loss: 0.0452, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [820/1000], Loss: 0.0451, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [830/1000], Loss: 0.0451, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [840/1000], Loss: 0.0451, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [850/1000], Loss: 0.0450, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [860/1000], Loss: 0.0450, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [870/1000], Loss: 0.0449, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [880/1000], Loss: 0.0449, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6960 (Best Test Acc: 0.7120)\n",
            "Epoch [890/1000], Loss: 0.0449, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [900/1000], Loss: 0.0448, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [910/1000], Loss: 0.0448, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [920/1000], Loss: 0.0447, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6980 (Best Test Acc: 0.7120)\n",
            "Epoch [930/1000], Loss: 0.0447, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [940/1000], Loss: 0.0447, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [950/1000], Loss: 0.0446, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [960/1000], Loss: 0.0446, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [970/1000], Loss: 0.0446, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [980/1000], Loss: 0.0445, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [990/1000], Loss: 0.0445, Train Acc: 0.9786, Val Acc: 0.6760, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n",
            "Epoch [1000/1000], Loss: 0.0445, Train Acc: 0.9786, Val Acc: 0.6780, Test Acc: 0.6970 (Best Test Acc: 0.7120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "dataset = Planetoid(root='data/Cora', name='Cora', transform=AddSelfLoops())\n",
        "graph_data = dataset[0]\n",
        "\n",
        "class MessagePassingLayer(nn.Module):\n",
        "    def __init__(self, num_nodes, in_features):\n",
        "        super(MessagePassingLayer, self).__init__()\n",
        "        self.Wk = nn.Parameter(torch.randn(in_features, in_features))\n",
        "        self.Wq = nn.Parameter(torch.randn(in_features, in_features))\n",
        "\n",
        "        self.W1 = nn.Parameter(torch.randn(num_nodes, num_nodes))\n",
        "        self.W2 = nn.Parameter(torch.randn(num_nodes, num_nodes))\n",
        "\n",
        "    def forward(self, X, A):\n",
        "        K = X @ self.Wk\n",
        "        Q = X @ self.Wq\n",
        "\n",
        "        alpha = torch.tanh((Q @ K.T) / X.size(0))\n",
        "\n",
        "        Fp = torch.tanh((Q.T @ K) / X.size(1))\n",
        "\n",
        "        A_hat = A - torch.eye(A.size(0), device=A.device)\n",
        "        X = self.W1 @ X @ Fp + alpha @ A_hat @ self.W2 @ X @ Fp\n",
        "\n",
        "        return X\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, num_nodes, in_features, out_features, hidden_features=2048):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.layer1 = MessagePassingLayer(num_nodes, in_features)\n",
        "        self.layer2 = MessagePassingLayer(num_nodes, in_features)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        X, edge_index = data.x, data.edge_index\n",
        "        A = to_dense_adj(edge_index, max_num_nodes=data.num_nodes).squeeze(0).to(X.device)\n",
        "\n",
        "        X = F.relu(self.layer1(X, A))\n",
        "\n",
        "        X = self.layer2(X, A)\n",
        "\n",
        "        X = self.classifier(X)\n",
        "\n",
        "        return F.log_softmax(X, dim=1)\n",
        "\n",
        "def train_model():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GNNModel(graph_data.num_nodes, dataset.num_node_features, dataset.num_classes).to(device)\n",
        "    data = graph_data.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-3)\n",
        "    best_acc = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(200):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        _, pred = out.max(dim=1)\n",
        "        correct = int((pred[data.test_mask] == data.y[data.test_mask]).sum())\n",
        "        acc = correct / int(data.test_mask.sum())\n",
        "\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_model = model.state_dict()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}, Test Accuracy: {acc:.4f}')\n",
        "\n",
        "    torch.save(best_model, 'best_gnn_model.pth')\n",
        "    print(f'Best Test Accuracy: {best_acc:.4f}')\n",
        "\n",
        "train_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T3r4f2Qe0Jb",
        "outputId": "7b178993-ccb0-45bd-ee93-46f324012fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 302317600.0000, Test Accuracy: 0.1140\n",
            "Epoch 10, Loss: 351617581056.0000, Test Accuracy: 0.0900\n",
            "Epoch 20, Loss: 125249372160.0000, Test Accuracy: 0.1370\n",
            "Epoch 30, Loss: 59005935616.0000, Test Accuracy: 0.0890\n",
            "Epoch 40, Loss: 75456897024.0000, Test Accuracy: 0.0980\n",
            "Epoch 50, Loss: 12477578240.0000, Test Accuracy: 0.1310\n",
            "Epoch 60, Loss: 4536907264.0000, Test Accuracy: 0.1350\n",
            "Epoch 70, Loss: 1421262208.0000, Test Accuracy: 0.1130\n",
            "Epoch 80, Loss: 4294049280.0000, Test Accuracy: 0.1440\n",
            "Epoch 90, Loss: 2987838720.0000, Test Accuracy: 0.3190\n",
            "Epoch 100, Loss: 3104175872.0000, Test Accuracy: 0.1440\n",
            "Epoch 110, Loss: 408250464.0000, Test Accuracy: 0.0890\n",
            "Epoch 120, Loss: 832193600.0000, Test Accuracy: 0.0910\n",
            "Epoch 130, Loss: 237441728.0000, Test Accuracy: 0.3190\n",
            "Epoch 140, Loss: 395570336.0000, Test Accuracy: 0.0640\n",
            "Epoch 150, Loss: 1.9468, Test Accuracy: 0.1300\n",
            "Epoch 160, Loss: 1.9468, Test Accuracy: 0.1300\n",
            "Epoch 170, Loss: 1.9469, Test Accuracy: 0.1300\n",
            "Epoch 180, Loss: 1.9470, Test Accuracy: 0.1300\n",
            "Epoch 190, Loss: 1.9470, Test Accuracy: 0.1300\n",
            "Best Test Accuracy: 0.3190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1 = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
        "data=dataset_1[0]"
      ],
      "metadata": {
        "id": "64wbPF70ee-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/n-gao/pytorch-kfac.git\n",
        "%cd pytorch-kfac\n",
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aec6c4L8d-T7",
        "outputId": "af2924cd-4c6d-4c44-aa5c-43e979505c0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-kfac'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 161 (delta 42), reused 32 (delta 32), pack-reused 108 (from 1)\u001b[K\n",
            "Receiving objects: 100% (161/161), 196.36 KiB | 24.54 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "/content/pytorch-kfac\n",
            "running install\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating torch_kfac.egg-info\n",
            "writing torch_kfac.egg-info/PKG-INFO\n",
            "writing dependency_links to torch_kfac.egg-info/dependency_links.txt\n",
            "writing top-level names to torch_kfac.egg-info/top_level.txt\n",
            "writing manifest file 'torch_kfac.egg-info/SOURCES.txt'\n",
            "reading manifest file 'torch_kfac.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'torch_kfac.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/lib/torch_kfac\n",
            "copying torch_kfac/kfac_optimizer.py -> build/lib/torch_kfac\n",
            "copying torch_kfac/__init__.py -> build/lib/torch_kfac\n",
            "creating build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/utils.py -> build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/conv_block.py -> build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/__init__.py -> build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/identity.py -> build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/fisher_block.py -> build/lib/torch_kfac/layers\n",
            "copying torch_kfac/layers/linear_block.py -> build/lib/torch_kfac/layers\n",
            "creating build/lib/torch_kfac/utils\n",
            "copying torch_kfac/utils/utils.py -> build/lib/torch_kfac/utils\n",
            "copying torch_kfac/utils/lock.py -> build/lib/torch_kfac/utils\n",
            "copying torch_kfac/utils/context_variable.py -> build/lib/torch_kfac/utils\n",
            "copying torch_kfac/utils/__init__.py -> build/lib/torch_kfac/utils\n",
            "copying torch_kfac/utils/moving_average.py -> build/lib/torch_kfac/utils\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/torch_kfac\n",
            "creating build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/utils.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/conv_block.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/__init__.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/identity.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/fisher_block.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/layers/linear_block.py -> build/bdist.linux-x86_64/egg/torch_kfac/layers\n",
            "copying build/lib/torch_kfac/kfac_optimizer.py -> build/bdist.linux-x86_64/egg/torch_kfac\n",
            "copying build/lib/torch_kfac/__init__.py -> build/bdist.linux-x86_64/egg/torch_kfac\n",
            "creating build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "copying build/lib/torch_kfac/utils/utils.py -> build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "copying build/lib/torch_kfac/utils/lock.py -> build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "copying build/lib/torch_kfac/utils/context_variable.py -> build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "copying build/lib/torch_kfac/utils/__init__.py -> build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "copying build/lib/torch_kfac/utils/moving_average.py -> build/bdist.linux-x86_64/egg/torch_kfac/utils\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/conv_block.py to conv_block.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/identity.py to identity.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/fisher_block.py to fisher_block.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/layers/linear_block.py to linear_block.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/kfac_optimizer.py to kfac_optimizer.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/utils/utils.py to utils.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/utils/lock.py to lock.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/utils/context_variable.py to context_variable.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/utils/__init__.py to __init__.cpython-310.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/torch_kfac/utils/moving_average.py to moving_average.cpython-310.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_kfac.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_kfac.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_kfac.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying torch_kfac.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/torch_kfac-0.0.1-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing torch_kfac-0.0.1-py3.10.egg\n",
            "Copying torch_kfac-0.0.1-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
            "Adding torch-kfac 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.10/dist-packages/torch_kfac-0.0.1-py3.10.egg\n",
            "Processing dependencies for torch-kfac==0.0.1\n",
            "Finished processing dependencies for torch-kfac==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv, GCNConv\n",
        "\n",
        "class GATGCNModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_dim=256, num_heads=16, dropout=0.4):\n",
        "        super(GATGCNModel, self).__init__()\n",
        "        self.gat1 = GATConv(in_channels, hidden_dim, heads=num_heads, dropout=dropout)\n",
        "        self.gcn2 = GCNConv(hidden_dim * num_heads, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = nn.functional.relu(self.gat1(x, edge_index))\n",
        "        x = nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.gcn2(x, edge_index)\n",
        "        return nn.functional.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, data, optimizer, kfac, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "\n",
        "    kfac.step()\n",
        "\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    accs = []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "hidden_dims = [16, 32, 64, 128, 256]\n",
        "lrs = [0.01, 0.015, 0.02]\n",
        "num_heads_list = [4, 8, 16]\n",
        "dropouts = [0.3, 0.4, 0.5]\n",
        "weight_decays = [5e-1, 5e-2, 5e-3, 5e-4]\n",
        "\n",
        "results = []\n",
        "\n",
        "num_runs = 30\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "    for lr in lrs:\n",
        "        for num_heads in num_heads_list:\n",
        "            for dropout in dropouts:\n",
        "                for weight_decay in weight_decays:\n",
        "                    best_accuracies = []\n",
        "\n",
        "                    for run in range(num_runs):\n",
        "                        if 'COLAB_TPU_ADDR' in os.environ:\n",
        "                          device = xm.xla_device()\n",
        "                        elif torch.cuda.is_available():\n",
        "                          device = torch.device(\"cuda\")\n",
        "                        else:\n",
        "                          device = torch.device(\"cpu\")\n",
        "                        print(f'Using device: {device}')\n",
        "                        print(run)\n",
        "                        model = GATGCNModel(dataset_1.num_features, dataset_1.num_classes, hidden_dim, num_heads, dropout).to(device)\n",
        "                        data = data.to(device)\n",
        "\n",
        "                        loss_fn = nn.NLLLoss()\n",
        "                        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "                        kfac = torch_kfac.KFAC(model, learning_rate=0.01, damping=0.001)\n",
        "\n",
        "                        best_test_acc = 0.0\n",
        "\n",
        "                        for epoch in range(50):\n",
        "                            loss = train(model, data, optimizer, kfac, loss_fn)\n",
        "                            _, _, test_acc = test(model, data)\n",
        "\n",
        "                            if test_acc > best_test_acc:\n",
        "                                best_test_acc = test_acc\n",
        "\n",
        "                        best_accuracies.append(best_test_acc)\n",
        "\n",
        "                    avg_best_accuracy = np.mean(best_accuracies)\n",
        "                    std_best_accuracy = np.std(best_accuracies)\n",
        "\n",
        "                    results.append({\n",
        "                        'hidden_dim': hidden_dim,\n",
        "                        'lr': lr,\n",
        "                        'num_heads': num_heads,\n",
        "                        'dropout': dropout,\n",
        "                        'weight_decay': weight_decay,\n",
        "                        'avg_best_accuracy': avg_best_accuracy,\n",
        "                        'std_best_accuracy': std_best_accuracy\n",
        "                    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(df_results)\n",
        "df_results.to_csv(\"gatgcn_hyperparam_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Iq96KqI_c6Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pu4M7MN0flUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c94bzTGwgp-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}